{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a381e43",
   "metadata": {},
   "source": [
    " Copyright © Sorbonne University.\n",
    "\n",
    " This source code is licensed under the MIT license found in the LICENSE file\n",
    " in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d9e603",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook we code the Soft Actor-Critic (SAC) algorithm using BBRL.\n",
    "This algorithm is described in [this\n",
    "paper](http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf) and [this\n",
    "paper](https://arxiv.org/pdf/1812.05905.pdf).\n",
    "\n",
    "To understand this code, you need to know more about [the BBRL interaction\n",
    "model](https://github.com/osigaud/bbrl/blob/master/docs/overview.md) Then you\n",
    "should run [a didactical\n",
    "example](https://github.com/osigaud/bbrl/blob/master/docs/notebooks/03-multi_env_autoreset.student.ipynb)\n",
    "to see how agents interact in BBRL when autoreset=True.\n",
    "\n",
    "The algorithm is explained in [this\n",
    "video](https://www.youtube.com/watch?v=U20F-MvThjM) and you can also read [the\n",
    "corresponding slides](http://pages.isir.upmc.fr/~sigaud/teach/ps/12_sac.pdf).\n",
    "\n",
    "\n",
    "# Setting up the environment\n",
    "We first need to setup the environment\n",
    "Installs the necessary Python and system libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79a474f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from easypip import easyimport\n",
    "except ModuleNotFoundError:\n",
    "    from subprocess import run\n",
    "\n",
    "    assert (\n",
    "        run([\"pip\", \"install\", \"easypip\"]).returncode == 0\n",
    "    ), \"Could not install easypip\"\n",
    "    from easypip import easyimport\n",
    "\n",
    "easyimport(\"swig\")\n",
    "easyimport(\"bbrl_utils>=0.5\").setup()\n",
    "\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bbrl.workspace import Workspace\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent, KWAgentWrapper\n",
    "from bbrl_utils.algorithms import EpochBasedAlgo\n",
    "from bbrl_utils.nn import build_mlp, setup_optimizer, soft_update_params\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from omegaconf import OmegaConf\n",
    "from torch.distributions import (\n",
    "    Normal,\n",
    "    Independent,\n",
    "    TransformedDistribution,\n",
    "    TanhTransform,\n",
    ")\n",
    "import bbrl_gymnasium  # noqa: F401"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459deec4",
   "metadata": {},
   "source": [
    "# Learning environment\n",
    "\n",
    "## Configuration\n",
    "\n",
    "The learning environment is controlled by a configuration that define a few\n",
    "important things as described in the example below. This configuration can\n",
    "hold as many extra information as you need, the example below is the minimal\n",
    "one.\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    # This defines the a path for logs and saved models\n",
    "    \"base_dir\": \"${gym_env.env_name}/myalgo_${current_time:}\",\n",
    "\n",
    "    # The Gymnasium environment\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPoleContinuous-v1\",\n",
    "    },\n",
    "\n",
    "    # Algorithm\n",
    "    \"algorithm\": {\n",
    "        # Seed used for the random number generator\n",
    "        \"seed\": 1023,\n",
    "\n",
    "        # Number of parallel training environments\n",
    "        \"n_envs\": 8,\n",
    "                \n",
    "        # Minimum number of steps between two evaluations\n",
    "        \"eval_interval\": 500,\n",
    "        \n",
    "        # Number of parallel evaluation environments\n",
    "        \"nb_evals\": 10,\n",
    "\n",
    "        # Number of epochs (loops)\n",
    "        \"max_epochs\": 40000,\n",
    "\n",
    "        # Number of steps (partial iteration)\n",
    "        \"n_steps\": 100,\n",
    "        \n",
    "    },\n",
    "}\n",
    "\n",
    "# Creates the configuration object, i.e. cfg.algorithm.nb_evals is 10\n",
    "cfg = OmegaConf.create(params)\n",
    "```\n",
    "\n",
    "## The RL algorithm\n",
    "\n",
    "In this notebook, the RL algorithm is based on `EpisodicAlgo`, that defines\n",
    "the algorithm environment when using episodes. To use such environment, we\n",
    "just need to subclass `EpisodicAlgo` and to define two things, namely the\n",
    "`train_policy` and the `eval_policy`. Both are BBRL agents that, given the\n",
    "environment state, select the action to perform.\n",
    "\n",
    "```py\n",
    "  class MyAlgo(EpisodicAlgo):\n",
    "      def __init__(self, cfg):\n",
    "          super().__init__(cfg)\n",
    "\n",
    "          # Define the train and evaluation policies\n",
    "          # (the agents compute the workspace `action` variable)\n",
    "          self.train_policy = MyPolicyAgent(...)\n",
    "          self.eval_policy = MyEvalAgent(...)\n",
    "\n",
    "algo = MyAlgo(cfg)\n",
    "```\n",
    "\n",
    "The `EpisodicAlgo` defines useful objects:\n",
    "\n",
    "- `algo.cfg` is the configuration\n",
    "- `algo.nb_steps` (integer) is the number of steps since the training began\n",
    "- `algo.logger` is a logger that can be used to collect statistics during training:\n",
    "    - `algo.logger.add_log(\"critic_loss\", critic_loss, algo.nb_steps)` registers the `critic_loss` value on tensorboard\n",
    "- `algo.evaluate()` evaluates the current `eval_policy` if needed, and keeps the\n",
    "agent if it was the best so far (average cumulated reward);\n",
    "- `algo.visualize_best()` runs the best agent on one episode, and displays the video\n",
    "\n",
    "\n",
    "\n",
    "Besides, it also defines an `iter_episodes` that allows to iterate over partial\n",
    "episodes (with `n_steps` from `n_envs` environments):\n",
    "\n",
    "```python3\n",
    "  # with partial episodes\n",
    "  for workspace in algo.iter_partial_episodes():\n",
    "      # workspace is a workspace containing 50 transitions\n",
    "      # (with autoreset)\n",
    "      ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceeeb27",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## The SquashedGaussianActor\n",
    "\n",
    "SAC works better with a Squashed Gaussian actor, which transforms a gaussian\n",
    "distribution with a $tanh$. The computation of the gradient  uses the\n",
    "reparametrization trick. Note that our attempts to use a\n",
    "`TunableVarianceContinuousActor` as we did for instance in the notebook about\n",
    "PPO completely failed. Such failure is also documented in the [OpenAI spinning\n",
    "up documentation page about\n",
    "SAC](https://spinningup.openai.com/en/latest/algorithms/sac.html).\n",
    "\n",
    "The code of the `SquashedGaussianActor` actor is below.\n",
    "\n",
    "The fact that we use the reparametrization trick is hidden inside the code of\n",
    "this distribution. You can read more about the reparametrization trick in at\n",
    "the following URLs:\n",
    "- [Goker Erdogan's\n",
    "  blog](http://gokererdogan.github.io/2016/07/01/reparameterization-trick/)\n",
    "  which shows the variance of different tricks to compute gradient of\n",
    "  expectations for $\\mathbb{E}(x^2)$ where $x \\sim \\mathcal{N}(\\theta, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b30d1873",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class SquashedGaussianActor(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim, min_std=1e-4):\n",
    "        \"\"\"Creates a new Squashed Gaussian actor\n",
    "\n",
    "        :param state_dim: The dimension of the state space\n",
    "        :param hidden_layers: Hidden layer sizes\n",
    "        :param action_dim: The dimension of the action space\n",
    "        :param min_std: The minimum standard deviation, defaults to 1e-4\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.min_std = min_std\n",
    "        backbone_dim = [state_dim] + list(hidden_layers)\n",
    "        self.layers = build_mlp(backbone_dim, activation=nn.ReLU())\n",
    "        self.backbone = nn.Sequential(*self.layers)\n",
    "        self.last_mean_layer = nn.Linear(hidden_layers[-1], action_dim)\n",
    "        self.last_std_layer = nn.Linear(hidden_layers[-1], action_dim)\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "        # cache_size avoids numerical infinites or NaNs when\n",
    "        # computing log probabilities\n",
    "        self.tanh_transform = TanhTransform(cache_size=1)\n",
    "\n",
    "    def normal_dist(self, obs: torch.Tensor):\n",
    "        \"\"\"Compute normal distribution given observation(s)\"\"\"\n",
    "        \n",
    "        backbone_output = self.backbone(obs)\n",
    "        mean = self.last_mean_layer(backbone_output)\n",
    "        std_out = self.last_std_layer(backbone_output)\n",
    "        std = self.softplus(std_out) + self.min_std\n",
    "        # Independent ensures that we have a multivariate\n",
    "        # Gaussian with a diagonal covariance matrix (given as\n",
    "        # a vector `std`)\n",
    "        return Independent(Normal(mean, std), 1)\n",
    "\n",
    "    def forward(self, t, stochastic=True):\n",
    "        \"\"\"Computes the action a_t and its log-probability p(a_t| s_t)\n",
    "\n",
    "        :param stochastic: True when sampling\n",
    "        \"\"\"\n",
    "        normal_dist = self.normal_dist(self.get((\"env/env_obs\", t)))\n",
    "        action_dist = TransformedDistribution(normal_dist, [self.tanh_transform])\n",
    "        if stochastic:\n",
    "            # Uses the re-parametrization trick\n",
    "            action = action_dist.rsample()\n",
    "        else:\n",
    "            # Directly uses the mode of the distribution\n",
    "            action = self.tanh_transform(normal_dist.mode)\n",
    "\n",
    "        log_prob = action_dist.log_prob(action)\n",
    "        # This line allows to deepcopy the actor...\n",
    "        self.tanh_transform._cached_x_y = [None, None]\n",
    "        self.set((\"action\", t), action)\n",
    "        self.set((\"action_logprobs\", t), log_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c22206d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Critic agent Q(s,a)\n",
    "\n",
    "As critics and target critics, SAC uses several instances of ContinuousQAgent\n",
    "class, as DDPG and TD3. See the [DDPG\n",
    "notebook](http://master-dac.isir.upmc.fr/rld/rl/04-ddpg-td3.student.ipynb) for\n",
    "details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dc68c0d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ContinuousQAgent(Agent):\n",
    "    def __init__(self, state_dim: int, hidden_layers: list[int], action_dim: int):\n",
    "        \"\"\"Creates a new critic agent $Q(s, a)$\n",
    "\n",
    "        :param state_dim: The number of dimensions for the observations\n",
    "        :param hidden_layers: The list of hidden layers for the NN\n",
    "        :param action_dim: The numer of dimensions for actions\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.is_q_function = True\n",
    "        self.model = build_mlp(\n",
    "            [state_dim + action_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        action = self.get((\"action\", t))\n",
    "        obs_act = torch.cat((obs, action), dim=1)\n",
    "        q_value = self.model(obs_act).squeeze(-1)\n",
    "        self.set((f\"{self.prefix}q_value\", t), q_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8000cf30",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Building the complete training and evaluation agents\n",
    "\n",
    "In the code below we create the Squashed Gaussian actor, two critics and the\n",
    "corresponding target critics. Beforehand, we checked that the environment\n",
    "takes continuous actions (otherwise we would need a different code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8878a0d8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create the SAC algorithm environment\n",
    "class SACAlgo(EpochBasedAlgo):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "\n",
    "        obs_size, act_size = self.train_env.get_obs_and_actions_sizes()\n",
    "        assert (\n",
    "            self.train_env.is_continuous_action()\n",
    "        ), \"SAC code dedicated to continuous actions\"\n",
    "\n",
    "        # We need an actor\n",
    "        self.actor = SquashedGaussianActor(\n",
    "            obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n",
    "        )\n",
    "\n",
    "        # Builds the critics\n",
    "        self.critic_1 = ContinuousQAgent(\n",
    "            obs_size,\n",
    "            cfg.algorithm.architecture.critic_hidden_size,\n",
    "            act_size,\n",
    "        ).with_prefix(\"critic-1/\")\n",
    "        self.target_critic_1 = copy.deepcopy(self.critic_1).with_prefix(\n",
    "            \"target-critic-1/\"\n",
    "        )\n",
    "\n",
    "        self.critic_2 = ContinuousQAgent(\n",
    "            obs_size,\n",
    "            cfg.algorithm.architecture.critic_hidden_size,\n",
    "            act_size,\n",
    "        ).with_prefix(\"critic-2/\")\n",
    "        self.target_critic_2 = copy.deepcopy(self.critic_2).with_prefix(\n",
    "            \"target-critic-2/\"\n",
    "        )\n",
    "\n",
    "        # Train and evaluation policies\n",
    "        self.train_policy = self.actor\n",
    "        self.eval_policy = KWAgentWrapper(self.actor, stochastic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b84789",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "For the entropy coefficient optimizer, the code is as follows. Note the trick\n",
    "which consists in using the log of this entropy coefficient. This trick was\n",
    "taken from the Stable baselines3 implementation of SAC, which is explained in\n",
    "[this\n",
    "notebook](https://colab.research.google.com/drive/12LER1_ShWOa_UhOL1nlX-LX_t5KQK9LV?usp=sharing).\n",
    "\n",
    "Tuning $\\alpha$ in SAC is an option. To chose to tune it, the `target_entropy`\n",
    "argument in the parameters should be `auto`. The initial value is given\n",
    "through the `entropy_coef` parameter. For any other value than `auto`, the\n",
    "value of $\\alpha$ will stay constant and correspond to the `entropy_coef`\n",
    "parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "788e5da8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def setup_entropy_optimizers(cfg):\n",
    "    if cfg.algorithm.entropy_mode == \"auto\":\n",
    "        # Note: we optimize the log of the entropy coef which is slightly different from the paper\n",
    "        # as discussed in https://github.com/rail-berkeley/softlearning/issues/37\n",
    "        # Comment and code taken from the SB3 version of SAC\n",
    "        log_entropy_coef = nn.Parameter(\n",
    "            torch.log(torch.ones(1) * cfg.algorithm.init_entropy_coef)\n",
    "        )\n",
    "        entropy_coef_optimizer = setup_optimizer(\n",
    "            cfg.entropy_coef_optimizer, log_entropy_coef\n",
    "        )\n",
    "        return entropy_coef_optimizer, log_entropy_coef\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a7434f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Compute the critic loss\n",
    "\n",
    "With the notations of my slides, the equation corresponding to Eq. (5) and (6)\n",
    "in [this paper](https://arxiv.org/pdf/1812.05905.pdf) becomes:\n",
    "\n",
    "$$ loss_{Q_{\\boldsymbol{\\phi}_i}}({\\boldsymbol{\\theta}}) = {\\mathbb{E}}_{(\\mathbf{s}_t, \\mathbf{a}_t, \\mathbf{s}_{t+1}) \\sim\n",
    "\\mathcal{D}}\\left[\\left( r(\\mathbf{s}_t, \\mathbf{a}_t) + \\gamma {\\mathbb{E}}_{\\mathbf{a} \\sim\n",
    "\\pi_{\\boldsymbol{\\theta}}(.|\\mathbf{s}_{t+1})} \\left[\n",
    "\\min_{j\\in 1,2} \\hat{Q}^{\\mathrm{target}}_{\\boldsymbol{\\phi}_j}(\\mathbf{s}_{t+1}, \\mathbf{a}) - \\alpha\n",
    "\\log{\\pi_{\\boldsymbol{\\theta}}(\\mathbf{a}|\\mathbf{s}_{t+1})} \\right] - \\hat{Q}_{\\boldsymbol{\\phi}_i}(\\mathbf{s}_t, \\mathbf{a}_t) \\right)^2\n",
    "\\right] $$\n",
    "\n",
    "An important information in the above equation and the one about the actor\n",
    "loss below is the index of the expectations. These indexes tell us where the\n",
    "data should be taken from. In the above equation, one can see that the index\n",
    "of the outer expectation is over samples taken from the replay buffer, whereas\n",
    "in the inner expectation we consider actions from the current actor at the\n",
    "next state $s_{t+1}$.\n",
    "\n",
    "Thus, to compute the inner expectation, one needs to determine what actions\n",
    "the current actor would take in the next state of each sample. This is what\n",
    "the line\n",
    "\n",
    "`t_actor(rb_workspace, t=1, n_steps=1, stochastic=True)`\n",
    "\n",
    "does. The parameter `t=1` (instead of 0) ensures that we consider the next\n",
    "state $s_{t+1}$.\n",
    "\n",
    "Once we have determined these actions, we can determine their Q-values and\n",
    "their log probabilities, to compute the inner expectation.\n",
    "\n",
    "Note that at this stage, we only determine the log probabilities corresponding\n",
    "to actions taken at the next time step, by contrast with what we do for the\n",
    "actor in the `compute_actor_loss(...)` function later on.\n",
    "\n",
    "Finally, once we have computed the $$\n",
    "\\hat{Q}_{\\boldsymbol{\\phi}}(\\mathbf{s}_{t+1},\n",
    "\\mathbf{a}) $$ for both critics, we take the min and store it into\n",
    "`post_q_values`. By contrast, the Q-values corresponding to the last term of\n",
    "the equation are taken from the replay buffer, they are computed in the\n",
    "beginning of the function by applying the Q agents to the replay buffer\n",
    "*before* changing the action to that of the current actor.\n",
    "\n",
    "An important remark is that, if the entropy coefficient $\\alpha$ corresponding\n",
    "to the `ent_coef` variable is set to 0, then we retrieve exactly the critic\n",
    "loss computation function of the TD3 algorithm. As we will see later, this is\n",
    "also true of the actor loss computation.\n",
    "\n",
    "This remark proved very useful in debugging the SAC code. We have set\n",
    "`ent_coef` to 0 and ensured the behavior was strictly the same as the behavior\n",
    "of TD3.\n",
    "\n",
    "Note also that we compute the loss for two critics (initialized\n",
    "independently), and use two target critics (using the minimum of their\n",
    "prediction as the basis of the target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92bc2e8a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def compute_critic_loss(\n",
    "    cfg,\n",
    "    reward: torch.Tensor,\n",
    "    must_bootstrap: torch.Tensor,\n",
    "    t_actor: TemporalAgent,\n",
    "    t_q_agents: TemporalAgent,\n",
    "    t_target_q_agents: TemporalAgent,\n",
    "    rb_workspace: Workspace,\n",
    "    ent_coef: torch.Tensor,\n",
    "):\n",
    "    r\"\"\"Computes the critic loss for a set of $S$ transition samples\n",
    "\n",
    "    Args:\n",
    "        cfg: The experimental configuration\n",
    "        reward: Tensor (2xS) of rewards\n",
    "        must_bootstrap: Tensor (2xS) of indicators\n",
    "        t_actor: The actor agent\n",
    "        t_q_agents: The critics\n",
    "        t_target_q_agents: The target of the critics\n",
    "        rb_workspace: The transition workspace\n",
    "        ent_coef: The entropy coefficient $\\alpha$\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: The two critic losses (scalars)\n",
    "    \"\"\"\n",
    "\n",
    "    # Replay the actor so we get the necessary statistics\n",
    "\n",
    "    t_q_agents(rb_workspace, t=0, n_steps=1)\n",
    "    with torch.no_grad():\n",
    "        t_actor(rb_workspace, t=1, n_steps=1)\n",
    "        \n",
    "        action_logprobs = rb_workspace[\"action_logprobs\"]\n",
    "        t_target_q_agents(rb_workspace, t=1, n_steps=1)\n",
    "        \n",
    "    q_value_1, q_value_2, q_values_next_1, q_values_next_2 = rb_workspace[\"critic-1/q_value\", \"critic-2/q_value\", \"target-critic-1/q_value\", \"target-critic-2/q_value\"]\n",
    "\n",
    "    # Compute temporal difference\n",
    "\n",
    "    q_values_next = torch.minimum(q_values_next_1[1], q_values_next_2[1])\n",
    "    esperance_interne = q_values_next - ent_coef*action_logprobs[1]\n",
    "    \n",
    "    target = reward[-1] + cfg.algorithm.discount_factor*esperance_interne*must_bootstrap.int()\n",
    "    \n",
    "    mse = nn.MSELoss()\n",
    "    \n",
    "    critic_loss_1 = mse(q_value_1[0].squeeze(-1), target)\n",
    "    critic_loss_2 = mse(q_value_2[0].squeeze(-1), target)\n",
    "\n",
    "    return critic_loss_1, critic_loss_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3bc70a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Compute the actor Loss\n",
    "\n",
    "With the notations of my slides, the equation of the actor loss corresponding\n",
    "to Eq. (7) in [this paper](https://arxiv.org/pdf/1812.05905.pdf) becomes:\n",
    "\n",
    "$$ loss_\\pi({\\boldsymbol{\\theta}}) = {\\mathbb{E}}_{\\mathbf{s}_t \\sim\n",
    "\\mathcal{D}}\\left[ {\\mathbb{E}}_{\\mathbf{a}_t\\sim\n",
    "\\pi_{\\boldsymbol{\\theta}}(.|\\mathbf{s}_t)} \\left[ \\alpha\n",
    "\\log{\\pi_{\\boldsymbol{\\theta}}(\\mathbf{a}_t|\\mathbf{s}_t) -\n",
    "\\hat{Q}_{\\boldsymbol{\\phi}_{i}}(\\mathbf{s}_t,\n",
    "\\mathbf{a}_t)} \\right] \\right] $$\n",
    "\n",
    "Note that [the paper](https://arxiv.org/pdf/1812.05905.pdf) mistakenly writes\n",
    "$Q_\\theta(s_t,s_t)$\n",
    "\n",
    "As for the critic loss, we have two expectations, one over the states from the\n",
    "replay buffer, and one over the actions of the current actor. Thus we need to\n",
    "apply again the current actor to the content of the replay buffer.\n",
    "\n",
    "But this time, we consider the current state, thus we parametrize it with\n",
    "`t=0` and `n_steps=1`. This way, we get the log probabilities and Q-values at\n",
    "the current step.\n",
    "\n",
    "A nice thing is that this way, there is no overlap between the log probability\n",
    "data used to update the critic and the actor, which avoids having to 'retain'\n",
    "the computation graph so that it can be reused for the actor and the critic.\n",
    "\n",
    "This small trick is one of the features that makes coding SAC the most\n",
    "difficult.\n",
    "\n",
    "Again, once we have computed the Q values over both critics, we take the min\n",
    "and put it into `current_q_values`.\n",
    "\n",
    "As for the critic loss, if we set `ent_coef` to 0, we retrieve the actor loss\n",
    "function of DDPG and TD3, which simply tries to get actions that maximize the\n",
    "Q values (by minimizing -Q)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43e31ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_actor_loss(\n",
    "    ent_coef, t_actor: TemporalAgent, t_q_agents: TemporalAgent, rb_workspace: Workspace\n",
    "):\n",
    "    r\"\"\"\n",
    "    Actor loss computation\n",
    "    :param ent_coef: The entropy coefficient $\\alpha$\n",
    "    :param t_actor: The actor agent (temporal agent)\n",
    "    :param t_q_agents: The critics (as temporal agent)\n",
    "    :param rb_workspace: The replay buffer (2 time steps, $t$ and $t+1$)\n",
    "    \"\"\"\n",
    "\n",
    "    # Recompute the action with the current actor (at $a_t$)\n",
    "\n",
    "    t_actor(rb_workspace, t=0, n_steps=1)\n",
    "    action_logprobs = rb_workspace[\"action_logprobs\"]\n",
    "    \n",
    "    t_q_agents(rb_workspace, t=0, n_steps=1)\n",
    "    q_values_1, q_values_2 = rb_workspace[\"critic-1/q_value\", \"critic-2/q_value\"]\n",
    "\n",
    "\n",
    "    # Compute Q-values\n",
    "\n",
    "    current_q_values = torch.minimum(q_values_1, q_values_2)\n",
    "\n",
    "    # Compute the actor loss\n",
    "    \n",
    "    actor_loss = action_logprobs[0]*ent_coef - current_q_values[0]\n",
    "\n",
    "\n",
    "\n",
    "    return actor_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe0257e",
   "metadata": {},
   "source": [
    "## Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9134daf6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def run_sac(sac: SACAlgo):\n",
    "    cfg = sac.cfg\n",
    "    logger = sac.logger\n",
    "\n",
    "\n",
    "    # init_entropy_coef is the initial value of the entropy coef alpha.\n",
    "    ent_coef = cfg.algorithm.init_entropy_coef\n",
    "    tau = cfg.algorithm.tau_target\n",
    "\n",
    "    # Creates the temporal actors\n",
    "    t_actor = TemporalAgent(sac.train_policy)\n",
    "    t_q_agents = TemporalAgent(Agents(sac.critic_1, sac.critic_2))\n",
    "    t_target_q_agents = TemporalAgent(Agents(sac.target_critic_1, sac.target_critic_2))\n",
    "\n",
    "    # Configure the optimizer\n",
    "    actor_optimizer = setup_optimizer(cfg.actor_optimizer, sac.actor)\n",
    "    critic_optimizer = setup_optimizer(cfg.critic_optimizer, sac.critic_1, sac.critic_2)\n",
    "    entropy_coef_optimizer, log_entropy_coef = setup_entropy_optimizers(cfg)\n",
    "\n",
    "\n",
    "    # If entropy_mode is not auto, the entropy coefficient ent_coef remains\n",
    "    # fixed. Otherwise, computes the target entropy\n",
    "    if cfg.algorithm.entropy_mode == \"auto\":\n",
    "        # target_entropy is \\mathcal{H}_0 in the SAC and aplications paper.\n",
    "        target_entropy = -np.prod(sac.train_env.action_space.shape).astype(np.float32)\n",
    "\n",
    "    # Loops over successive replay buffers\n",
    "    for rb in sac.iter_replay_buffers():\n",
    "        rb_workspace = rb.get_shuffled(sac.cfg.algorithm.batch_size)\n",
    "        # Implement the SAC algorithm\n",
    "        terminated, reward = rb_workspace[\"env/terminated\", \"env/reward\"]\n",
    "        # Critic update part #############################\n",
    "        \n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss_1, critic_loss_2 = compute_critic_loss(cfg, reward, terminated[1], t_actor, t_q_agents, t_target_q_agents, rb_workspace, ent_coef)\n",
    "        \n",
    "        logger.add_log(\"critic_loss_1\", critic_loss_1, sac.nb_steps)\n",
    "        logger.add_log(\"critic_loss_2\", critic_loss_2, sac.nb_steps)\n",
    "        critic_loss = critic_loss_1 + critic_loss_2\n",
    "        print(\"critic_loss :\",critic_loss)\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            sac.critic_1.parameters(), cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            sac.critic_2.parameters(), cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        critic_optimizer.step()\n",
    "        \n",
    "        # Actor update part #############################\n",
    "\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss = compute_actor_loss(ent_coef, t_actor, t_q_agents, rb_workspace)\n",
    "        print(\"actor_loss :\",actor_loss)\n",
    "        sac.logger.add_log(\"actor_loss\", actor_loss, sac.nb_steps)\n",
    "\n",
    "        \n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            sac.train_policy.parameters(), sac.cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        actor_optimizer.step()\n",
    "\n",
    "        # Entropy optimizer part\n",
    "        if entropy_coef_optimizer is not None:\n",
    "            # See Eq. (17) of the SAC and Applications paper. The log\n",
    "            # probabilities *must* have been computed when computing the actor\n",
    "            # loss.\n",
    "            action_logprobs_rb = rb_workspace[\"action_logprobs\"].detach()\n",
    "            entropy_coef_loss = -(\n",
    "                log_entropy_coef.exp() * (action_logprobs_rb + target_entropy)\n",
    "            ).mean()\n",
    "            entropy_coef_optimizer.zero_grad()\n",
    "            entropy_coef_loss.backward()\n",
    "            entropy_coef_optimizer.step()\n",
    "            logger.add_log(\"entropy_coef_loss\", entropy_coef_loss, sac.nb_steps)\n",
    "            logger.add_log(\"entropy_coef\", torch.tensor(ent_coef), sac.nb_steps)\n",
    "\n",
    "        ####################################################\n",
    "\n",
    "        # Soft update of target q function\n",
    "        soft_update_params(sac.critic_1, sac.target_critic_1, tau)\n",
    "        soft_update_params(sac.critic_2, sac.target_critic_2, tau)\n",
    "\n",
    "        sac.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbea0a2",
   "metadata": {},
   "source": [
    "## Definition of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6b31c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"save_best\": True,\n",
    "    \"base_dir\": \"${gym_env.env_name}/sac-S${algorithm.seed}_${current_time:}\",\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 1,\n",
    "        \"n_envs\": 8,\n",
    "        \"n_steps\": 32,\n",
    "        \"buffer_size\": 1e6,\n",
    "        \"batch_size\": 256,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"nb_evals\": 16,\n",
    "        \"eval_interval\": 2_000,\n",
    "        \"learning_starts\": 10_000,\n",
    "        \"max_epochs\": 2_000,\n",
    "        \"discount_factor\": 0.98,\n",
    "        \"entropy_mode\": \"auto\",  # \"auto\" or \"fixed\"\n",
    "        \"init_entropy_coef\": 2e-7,\n",
    "        \"tau_target\": 0.05,\n",
    "        \"architecture\": {\n",
    "            \"actor_hidden_size\": [64, 64],\n",
    "            \"critic_hidden_size\": [256, 256],\n",
    "        },\n",
    "    },\n",
    "    \"gym_env\": {\"env_name\": \"CartPoleContinuous-v1\"},\n",
    "    \"actor_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 3e-4,\n",
    "    },\n",
    "    \"critic_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 3e-4,\n",
    "    },\n",
    "    \"entropy_coef_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 3e-4,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa392a40",
   "metadata": {},
   "source": [
    "## Launching tensorboard to visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f45903",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_tensorboard(\"./outputs/tblogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c93017d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321d002e6a5547019305933402fd834f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "critic_loss : tensor(2.3864, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(0.0716, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(2.1411, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(0.0092, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(1.9394, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-0.0432, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(1.7463, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-0.0974, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(1.5642, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-0.1510, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(1.3992, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-0.2007, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(1.2367, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-0.2511, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(1.0717, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-0.3054, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.9643, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-0.3431, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.8019, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-0.4090, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.7026, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-0.4510, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.5998, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-0.5005, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.4830, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-0.5533, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.4116, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-0.5994, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.3138, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-0.6717, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.2521, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-0.7238, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.2004, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-0.7627, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1468, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-0.8258, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1076, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-0.8485, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0806, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-0.9127, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0680, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-0.9911, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0503, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0214, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0640, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0852, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.1385, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0705, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.1422, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0717, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.1537, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0887, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.1522, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1015, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.1607, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0639, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.1263, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0856, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.1351, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0799, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.1028, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0772, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0965, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0598, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0627, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0626, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0352, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0713, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0176, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0782, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0097, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0600, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-0.9863, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0791, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-0.9909, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0629, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-0.9720, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0709, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-0.9967, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0941, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-0.9803, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0779, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0060, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0651, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0104, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0926, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0291, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0671, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0307, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0473, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0880, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0639, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0662, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0614, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0542, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0659, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0824, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0707, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0725, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0638, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0618, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0614, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0550, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0543, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1018, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0566, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0656, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0474, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1029, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0519, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0807, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0443, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0959, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0263, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1227, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0318, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0350, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0866, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0263, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0673, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0359, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0827, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0274, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1074, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0364, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0710, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0331, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0830, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0408, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0730, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0443, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0571, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0884, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0667, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0891, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0652, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0539, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0561, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1395, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0761, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0632, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0669, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1058, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0750, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0639, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1119, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0609, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0776, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0479, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0697, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0484, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0695, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0436, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0405, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1448, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0442, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0957, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0409, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0911, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0322, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1064, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0506, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0372, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0428, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0969, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0472, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0580, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0421, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0787, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0560, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1115, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0541, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0945, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0691, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0985, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0635, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1075, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0600, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1048, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0620, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0945, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0547, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0665, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0509, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0564, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0416, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1057, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0471, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0330, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1243, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0409, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0953, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0466, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0897, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0427, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1061, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0380, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0645, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0406, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0994, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0535, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0637, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1189, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0589, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0592, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0567, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0662, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0664, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1225, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0703, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0644, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0475, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1006, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0398, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1136, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0476, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0670, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0348, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0903, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0396, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0393, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0996, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0555, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0877, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0515, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0662, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0398, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0409, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0415, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1245, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0499, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1127, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0480, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0875, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0496, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0832, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0600, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.1475, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0777, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0659, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0733, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0666, grad_fn=<MeanBackward0>)\n",
      "critic_loss : tensor(0.0604, grad_fn=<AddBackward0>)\n",
      "actor_loss : tensor(-1.0505, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m agents \u001b[38;5;241m=\u001b[39m SACAlgo(OmegaConf\u001b[38;5;241m.\u001b[39mcreate(params))\n\u001b[1;32m----> 2\u001b[0m \u001b[43mrun_sac\u001b[49m\u001b[43m(\u001b[49m\u001b[43magents\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 31\u001b[0m, in \u001b[0;36mrun_sac\u001b[1;34m(sac)\u001b[0m\n\u001b[0;32m     28\u001b[0m     target_entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mprod(sac\u001b[38;5;241m.\u001b[39mtrain_env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Loops over successive replay buffers\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msac\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_replay_buffers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrb_workspace\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_shuffled\u001b[49m\u001b[43m(\u001b[49m\u001b[43msac\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Implement the SAC algorithm\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\simon\\miniconda3\\envs\\bima\\Lib\\site-packages\\bbrl_utils\\algorithms.py:295\u001b[0m, in \u001b[0;36mEpochBasedAlgo.iter_replay_buffers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    293\u001b[0m     train_workspace\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    294\u001b[0m     train_workspace\u001b[38;5;241m.\u001b[39mcopy_n_last_steps(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 295\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_workspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstochastic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;66;03m# Add transitions to buffer\u001b[39;00m\n\u001b[0;32m    303\u001b[0m transition_workspace \u001b[38;5;241m=\u001b[39m train_workspace\u001b[38;5;241m.\u001b[39mget_transitions()\n",
      "File \u001b[1;32mc:\\Users\\simon\\miniconda3\\envs\\bima\\Lib\\site-packages\\bbrl\\agents\\utils.py:79\u001b[0m, in \u001b[0;36mTemporalAgent.__call__\u001b[1;34m(self, workspace, t, n_steps, stop_variable, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m _t \u001b[38;5;241m=\u001b[39m t\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_variable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m         s \u001b[38;5;241m=\u001b[39m workspace\u001b[38;5;241m.\u001b[39mget(stop_variable, _t)\n",
      "File \u001b[1;32mc:\\Users\\simon\\miniconda3\\envs\\bima\\Lib\\site-packages\\bbrl\\agents\\utils.py:31\u001b[0m, in \u001b[0;36mAgents.__call__\u001b[1;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, workspace, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents:\n\u001b[1;32m---> 31\u001b[0m         \u001b[43ma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\simon\\miniconda3\\envs\\bima\\Lib\\site-packages\\bbrl\\agents\\agent.py:84\u001b[0m, in \u001b[0;36mAgent.__call__\u001b[1;34m(self, workspace, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m workspace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Agent.__call__] workspace must not be None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkspace \u001b[38;5;241m=\u001b[39m workspace\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkspace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 49\u001b[0m, in \u001b[0;36mSquashedGaussianActor.forward\u001b[1;34m(self, t, stochastic)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Directly uses the mode of the distribution\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtanh_transform(normal_dist\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m---> 49\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m \u001b[43maction_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# This line allows to deepcopy the actor...\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtanh_transform\u001b[38;5;241m.\u001b[39m_cached_x_y \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "File \u001b[1;32mc:\\Users\\simon\\miniconda3\\envs\\bima\\Lib\\site-packages\\torch\\distributions\\transformed_distribution.py:164\u001b[0m, in \u001b[0;36mTransformedDistribution.log_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03mScores the sample by inverting the transform(s) and computing the score\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03musing the score of the base distribution and the log abs det jacobian.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_args:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m event_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_shape)\n\u001b[0;32m    166\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\simon\\miniconda3\\envs\\bima\\Lib\\site-packages\\torch\\distributions\\distribution.py:314\u001b[0m, in \u001b[0;36mDistribution._validate_sample\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m support \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[43msupport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    317\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected value argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\simon\\miniconda3\\envs\\bima\\Lib\\site-packages\\torch\\distributions\\constraints.py:245\u001b[0m, in \u001b[0;36m_IndependentConstraint.check\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[0;32m    244\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m--> 245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreinterpreted_batch_ndims:\n\u001b[0;32m    246\u001b[0m         expected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_constraint\u001b[38;5;241m.\u001b[39mevent_dim \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreinterpreted_batch_ndims\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected value.dim() >= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m         )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agents = SACAlgo(OmegaConf.create(params))\n",
    "run_sac(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "575932a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video of best agent recorded in outputs\\CartPoleContinuous-v1\\sac-S1_20250312-141912\\best_agent.mp4\n",
      "Moviepy - Building video C:\\Users\\simon\\OneDrive\\Bureau\\ANDROIDE\\S2\\PANDROIDE\\Projet\\SAC\\outputs\\CartPoleContinuous-v1\\sac-S1_20250312-141912\\best_agent.mp4.\n",
      "Moviepy - Writing video C:\\Users\\simon\\OneDrive\\Bureau\\ANDROIDE\\S2\\PANDROIDE\\Projet\\SAC\\outputs\\CartPoleContinuous-v1\\sac-S1_20250312-141912\\best_agent.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\simon\\OneDrive\\Bureau\\ANDROIDE\\S2\\PANDROIDE\\Projet\\SAC\\outputs\\CartPoleContinuous-v1\\sac-S1_20250312-141912\\best_agent.mp4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div align=middle><video src='data:video/mp4;base64,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' controls>Sorry, seems like your browser doesn't support HTML5 audio/video</video></div>"
      ],
      "text/plain": [
       "<moviepy.video.io.html_tools.HTML2 object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the best policy\n",
    "agents.visualize_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abc6bc4",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "- use the same code on the Pendulum-v1 environment. This one is harder to\n",
    "  tune. Get the parameters from the\n",
    "  [rl-baseline3-zoo](https://github.com/DLR-RM/rl-baselines3-zoo) and see if\n",
    "  you manage to get SAC working on Pendulum"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "bima",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
